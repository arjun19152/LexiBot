{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \\\n",
        "  llama-index \\\n",
        "  llama-index-embeddings-huggingface \\\n",
        "  llama-index-llms-huggingface \\\n",
        "  transformers accelerate sentence-transformers pypdf bitsandbytes\n"
      ],
      "metadata": {
        "id": "5czmmcmgCv9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "-odf0HY_L69o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded_files = files.upload()\n"
      ],
      "metadata": {
        "id": "SzG96VyRCwYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "from llama_index.core import Document\n",
        "\n",
        "documents = []\n",
        "\n",
        "for filename in uploaded_files:\n",
        "    reader = PdfReader(filename)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() or \"\"\n",
        "\n",
        "    if text.strip():\n",
        "        documents.append(Document(text=text))\n",
        "\n",
        "print(f\"Loaded {len(documents)} document(s)\")\n"
      ],
      "metadata": {
        "id": "m58HTahxCxs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "Settings.embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-base-en-v1.5\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "WAlwlqo8CzfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "import torch\n",
        "\n",
        "Settings.llm = HuggingFaceLLM(\n",
        "    model_name=\"Qwen/Qwen2-7B-Instruct\",\n",
        "    tokenizer_name=\"Qwen/Qwen2-7B-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    model_kwargs={\n",
        "        \"load_in_4bit\": True,\n",
        "        \"bnb_4bit_compute_dtype\": torch.float16,\n",
        "        \"bnb_4bit_use_double_quant\": True,\n",
        "        \"bnb_4bit_quant_type\": \"nf4\",\n",
        "    },\n",
        "    generate_kwargs={\n",
        "        \"temperature\": 0.1\n",
        "    },\n",
        "    is_chat_model=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "1F5jPgdMC1FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.chunk_size = 512\n",
        "Settings.chunk_overlap = 50\n"
      ],
      "metadata": {
        "id": "jI_kUmkxC8LL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "print(\"Vector index built successfully\")\n"
      ],
      "metadata": {
        "id": "KsACQ3JVC90I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.prompts import PromptTemplate\n",
        "\n",
        "LEGAL_PROMPT = PromptTemplate(\n",
        "\"\"\"\n",
        "Answer ONLY using the provided context.\n",
        "If the answer is not present, reply exactly:\n",
        "\"The uploaded document does not contain information relevant to this query.\"\n",
        "\n",
        "Context:\n",
        "{context_str}\n",
        "\n",
        "Question:\n",
        "{query_str}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "7MuLWhU9C_Hb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.response_synthesizers import ResponseMode\n",
        "\n",
        "query_engine = index.as_query_engine(\n",
        "    similarity_top_k=5,\n",
        "    response_mode=ResponseMode.REFINE,\n",
        "    text_qa_template=LEGAL_PROMPT,\n",
        ")\n"
      ],
      "metadata": {
        "id": "GviYhst_DA_B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_lexibot(user_query):\n",
        "    if not user_query.strip():\n",
        "        return \"Please enter a valid question.\"\n",
        "\n",
        "    answer = str(query_engine.query(user_query)).strip()\n",
        "\n",
        "    if len(answer) < 40:\n",
        "        return \"The uploaded document does not contain information relevant to this query.\"\n",
        "\n",
        "    return answer\n",
        "\n"
      ],
      "metadata": {
        "id": "4s7qaSC2DCit"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks(title=\"LexiBot ⚖️\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # ⚖️ LexiBot\n",
        "        **Legal Document Assistant**\n",
        "        Upload documents above, then ask legal questions below.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    chatbot = gr.Chatbot(height=350)\n",
        "\n",
        "    user_input = gr.Textbox(\n",
        "        placeholder=\"Ask a legal question...\",\n",
        "        show_label=False\n",
        "    )\n",
        "\n",
        "    def respond(message, history):\n",
        "        response = chat_with_lexibot(message)\n",
        "        history.append((message, response))\n",
        "        return history, \"\"\n",
        "\n",
        "    user_input.submit(respond, [user_input, chatbot], [chatbot, user_input])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "tnA0IE9yDD-c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}